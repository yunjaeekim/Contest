{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 전체 코드를 정리해서 입력하세요.\n",
    "client_id = ''\n",
    "client_secret = ''\n",
    "base_url = 'https://openapi.naver.com/v1/search/news.json'\n",
    "query = '재생에너지'\n",
    "encQuery = urllib.parse.quote(query)\n",
    "n_display = 10\n",
    "start = 1\n",
    "sort = 'sim'\n",
    "url = f'{base_url}?query={encQuery}&display={n_display}&start={start}&sort={sort}'\n",
    "\n",
    "my_request = urllib.request.Request(url)\n",
    "my_request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "my_request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(my_request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "#     print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)\n",
    "search_results = response_body.decode('utf-8')\n",
    "search_results = eval(search_results)\n",
    "all_results = dict()\n",
    "i = 0\n",
    "for item in search_results['items']:\n",
    "    link =  html.unescape(item['link']).replace('\\\\','')\n",
    "    pubdate = item['pubDate']\n",
    "    if 'n.news.naver.com' in link:\n",
    "        all_results[i] = dict()\n",
    "        all_results[i]['link'] = link\n",
    "        all_results[i]['pubdate'] = pubdate\n",
    "        i += 1\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 페이지 로드를 위해 기다리는 시간\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "for j in range(len(all_results)):\n",
    "    \n",
    "    # scraping하려는 웹페이지 주소를 get()에 전달\n",
    "    driver.get(all_results[j]['link'])\n",
    "    time.sleep(1)\n",
    "    # 제목 추출하기\n",
    "    title = driver.find_elements(By.CLASS_NAME, 'media_end_head_headline')\n",
    "    title = title[0].text\n",
    "        \n",
    "    # 본문 추출하기\n",
    "    body = driver.find_elements(By.ID, 'newsct_article')\n",
    "    # body = driver.find_elements(By.ID, 'articeBody') # 가끔 ID가 이렇게 생긴 녀석도 존재합니다...\n",
    "    body = body[0].text.replace('\\n','')\n",
    "    all_results[j]['link'] = link\n",
    "    all_results[j]['title'] = title\n",
    "    all_results[j]['content'] = body\n",
    "df_all_comments = pd.DataFrame(all_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for cnt in range(10):\n",
    "    client_id = ''\n",
    "    client_secret = ''\n",
    "    base_url = 'https://openapi.naver.com/v1/search/news.json'\n",
    "    query = '지구온난화'\n",
    "    encQuery = urllib.parse.quote(query)\n",
    "    # 한 번에 표시할 검색 결과 개수(기본 값: 10, 최댓 값: 100)\n",
    "    n_display = 100\n",
    "    # 검색 시작 위치(기본 값: 1, 최댓 값: 1000)\n",
    "    start = 1 + 100*cnt\n",
    "    # 검색 결과 정렬 방법(sim: 정확도 순, date: 날짜 순)\n",
    "    sort = 'sim'\n",
    "    url = f'{base_url}?query={encQuery}&display={n_display}&start={start}&sort={sort}'\n",
    "\n",
    "    my_request = urllib.request.Request(url)\n",
    "    my_request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    my_request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    response = urllib.request.urlopen(my_request)\n",
    "    rescode = response.getcode()\n",
    "    #정상적으로 연결 됐으면 안에 내용을 읽고 아니면 Error code를 출력합니다.\n",
    "    if(rescode==200):\n",
    "        response_body = response.read()\n",
    "    #     print(response_body.decode('utf-8'))\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "    search_results = response_body.decode('utf-8')\n",
    "    #읽은 문자열들을 dictionary 형태로 반환합니다.\n",
    "    search_results = eval(search_results)\n",
    "    all_results = dict()\n",
    "    i = 0\n",
    "    for item in search_results['items']:\n",
    "        link =  html.unescape(item['link']).replace('\\\\','')\n",
    "        pubdate = item['pubDate']\n",
    "        # 해당 문자열이 link에 있을 때를 기준으로 데이터들을 분리합니다.(보다 편한 크롤링을 위해)\n",
    "        if 'n.news.naver.com' in link:\n",
    "            all_results[i] = dict()\n",
    "            all_results[i]['link'] = link\n",
    "            all_results[i]['pubdate'] = pubdate\n",
    "            i += 1\n",
    "    from selenium import webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # 페이지 로드를 위해 기다리는 시간\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    for j in range(len(all_results)):\n",
    "        \n",
    "        # scraping하려는 웹페이지 주소를 get()에 전달\n",
    "        driver.get(all_results[j]['link'])\n",
    "        time.sleep(1)\n",
    "        #제목과 본문을 추출합니다.\n",
    "        title = driver.find_elements(By.CLASS_NAME, 'media_end_head_headline')\n",
    "        body = driver.find_elements(By.ID, 'newsct_article')\n",
    "        #만약 제목과 본문에 내용이 없거나 누락되었다면(Index Error) 그냥 pass 하고\n",
    "        #HTTPS 접속이 어려워져 오류가 난다면(HTTP Error) pass 합니다.\n",
    "        try:\n",
    "            title = title[0].text\n",
    "            body = body[0].text.replace('\\n','')\n",
    "        except IndexError:\n",
    "            pass\n",
    "        except requests.exceptions.HTTPError:\n",
    "            pass\n",
    "        all_results[j]['link'] = link\n",
    "        all_results[j]['title'] = title\n",
    "        all_results[j]['content'] = body\n",
    "    df_all_comments_1 = pd.DataFrame(all_results).T\n",
    "    df_all_comments = pd.concat([df_all_comments,df_all_comments_1]).reset_index(drop=True)\n",
    "# 여러 번의 크롤링을 통해 전체 데이터 프레임이 만들어진다면 'content' 값을 기준으로 중복을 제거합니다.\n",
    "df_dust = df_all_comments.drop_duplicates(subset = ['content'],keep = 'first', inplace= False).reset_index(drop = True)\n",
    "idx_lst = []\n",
    "# 또한 'content' 값이 누락되거나 내용이 없는 경우(즉, None값) 해당 index를 제거합니다.\n",
    "for idx,i in enumerate(df_dust['content']):\n",
    "    if type(i) == list:\n",
    "        idx_lst.append(idx)\n",
    "df_dust.drop(index = idx_lst, inplace= True)\n",
    "# 마지막으로 결측치를 제거합니다.\n",
    "df_dust.dropna(inplace=True)\n",
    "#이 후 전처리가 완료된 csv 파일을 저장합니다.\n",
    "df_dust.to_csv('지구온난화_.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comments.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dust = df_all_comments.drop_duplicates(subset = ['content'],keep = 'first', inplace= False).reset_index(drop = True)\n",
    "idx_lst = []\n",
    "for idx,i in enumerate(df_dust['content']):\n",
    "    if type(i) == list:\n",
    "        idx_lst.append(idx)\n",
    "df_dust.drop(index = idx_lst, inplace= True)\n",
    "df_dust.dropna(inplace=True)\n",
    "df_dust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dust.to_csv('대체에너지.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''a = pd.read_csv(\"대기 오염 미세먼지_news.csv\",encoding = 'utf-8')\n",
    "a1 = pd.read_csv(\"미세먼지.csv\",encoding = 'utf-8')\n",
    "a2 = pd.read_csv(\"미세먼지_계절.csv\",encoding = 'utf-8')\n",
    "a3 = pd.read_csv(\"미세먼지_농도_news.csv\",encoding = 'utf-8')\n",
    "a4 = pd.read_csv(\"미세먼지_문제.csv\",encoding = 'utf-8')\n",
    "a5 = pd.read_csv(\"미세먼지_배출_사업장_news.csv\",encoding = 'utf-8')\n",
    "a6 = pd.read_csv(\"미세먼지_이유.csv\",encoding = 'utf-8')\n",
    "a7 = pd.read_csv(\"미세먼지_주요지역.csv\",encoding = 'utf-8')\n",
    "a8 = pd.read_csv(\"미세먼지_키워드.csv\",encoding = 'utf-8')\n",
    "a9 = pd.read_csv(\"미세먼지_해결_news.csv\",encoding = 'utf-8')\n",
    "a10 = pd.read_csv(\"미세먼지관련_정책_news.csv\",encoding = 'utf-8')\n",
    "a11 = pd.read_csv(\"초미세먼지_news.csv\",encoding = 'utf-8')\n",
    "a12 = pd.read_csv(\"초미세먼지_문제_news.csv\",encoding = 'utf-8')\n",
    "a13 = pd.read_csv(\"초미세먼지_원인_news.csv\",encoding = 'utf-8')\n",
    "a14 = pd.read_csv(\"초미세먼지_해결_news.csv\",encoding = 'utf-8')\n",
    "a15 = pd.read_csv(\"초미세먼지_해결_wordcloud_파생_.csv\",encoding = 'utf-8')\n",
    "a16 = pd.read_csv(\"환경문제_.csv\",encoding = 'utf-8')\n",
    "a17 = pd.read_csv(\"지구온난화_.csv\", encoding = 'utf-8')\n",
    "a18 = pd.read_csv(\"대기오염.csv\", encoding = 'utf-8')\n",
    "a19 = pd.read_csv(\"토양오염.csv\", encoding = 'utf-8')\n",
    "a20 = pd.read_csv(\"대체에너지.csv\", encoding= 'utf-8')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의사항 \n",
    "# ex)미세먼지 keyword \n",
    "# => 미세 먼지, 미세먼지 (이렇게 띄워쓰기 등을 활용하여 최대한 데이터를 많이 뽑게 진행)\n",
    "# 이로 인해 duplicate의 중요성이 더 강조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''환경 = pd.concat([a,a17,a1,a11,a8], axis = 0).reset_index(drop=True)\n",
    "환경문제 = pd.concat([a3,a4,a12,a16], axis = 0).reset_index(drop=True)\n",
    "환경해결 = pd.concat([a9,a14,a20])\n",
    "환경원인 = pd.concat([a6,a13], axis=0).reset_index(drop = True)\n",
    "wordcloud_파생_정책관련 = pd.concat([a2,a5,a7,a10],axis = 0).reset_index(drop=True)\n",
    "wordcloud_파생_오염관련 = pd.concat([a18,a19,a15],axis = 0).reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wordcloud_파생_정책관련.dropna(subset=['content'], how='any', axis=0, inplace = True)\n",
    "wordcloud_파생_정책관련 = wordcloud_파생_오염관련.drop_duplicates(keep = 'first', subset= ['content']).reset_index(drop = True)\n",
    "wordcloud_파생_정책관련.drop(columns = ['Unnamed: 0','link','pubdate'], inplace = True)\n",
    "\n",
    "wordcloud_파생_오염관련.dropna(subset=['content'], how='any', axis=0, inplace = True)\n",
    "wordcloud_파생_오염관련 = wordcloud_파생_오염관련.drop_duplicates(keep = 'first', subset= ['content']).reset_index(drop = True)\n",
    "wordcloud_파생_오염관련.drop(columns = ['Unnamed: 0','link','pubdate'], inplace = True);wordcloud_파생_오염관련'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "환경.to_csv('환경.csv', encoding = 'utf-8', index = False)\n",
    "환경문제.to_csv('환경문제.csv', encoding = 'utf-8', index = False)\n",
    "환경해결.to_csv('환경해결.csv', encoding = 'utf-8', index = False)\n",
    "환경원인.to_csv('환경원인.csv', encoding = 'utf-8', index = False)\n",
    "wordcloud_파생_정책관련.to_csv('wordcloud_파생_정책관련.csv', encoding = 'utf-8', index = False)\n",
    "wordcloud_파생_오염관련.to_csv('wordcloud_파생_오염관련.csv', encoding = 'utf-8', index = False)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
