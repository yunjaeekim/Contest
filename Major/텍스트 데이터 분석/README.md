## 🖥️ 환경 문제와 관련된 뉴스 기사 본문 WordCloud & Topic Modeling

### 📋 1. 프로젝트 개요
3학년 1학기 '텍스트 데이터 분석' 수업에서 진행한 프로젝트로, 자유 주제를 선정하여 텍스트 데이터 수집, 전처리, 분석까지 수행하는 팀 과제 입니다.
- 팀원 : 김윤재, 김상욱
- 일시 : 2024.04.01 ~ 2024.06.02

#### 📌 역할
- 뉴스 본문 크롤링 코드 작성 및 크롤링 진행
- 정규 표현식을 통한 텍스트 데이터 전처리 코드 작성
- 전처리 된 텍스트 데이터 word cloud 시각화

### 🎯 2. 주제 선정 과정
학교 내에서 기후변화 대응 사업단의 다양한 활동들을 지켜보며, 환경 관련 프로젝트를 진행하고자 했습니다. 이로 인해 '기후 환경 문제'를 주제로 선택하게 되었습니다.

### 🗂️ 3. 데이터 수집
- 네이버 뉴스 기사 크롤링을 통한 15000개의 뉴스 기사 본문 추출

### 🛠️ 4. 데이터 전처리
- unescape 문을 통해 HTML 언어를 문자열로 변환
- stopwords(불용어) 텍스트 문서를 이용하여 불용어 제거
- 정규 표현식을 통한 줄 바꿈 문자 제거
- pykospacing을 통해 띄어쓰기 교정
- pos taging을 통해 많은 품사 중에서 '명사'만 추출

### 📊 5. 데이터 시각화
- 각 단어들의 빈도를 계산하여 WordCloud 생성
<img src="https://github.com/yunjaeekim/Contest/assets/133327199/3b7024b7-4e41-46c5-bcc3-18f90b00067b" alt="Example Image" width="400" height="300"/>

- Bag-of-word를 통해 각 문서의 단어들을 벡터화 => pyLDAvis를 통해 각 문서의 토픽 15개 추출
- 
<img src="https://github.com/yunjaeekim/Contest/assets/133327199/6ffe95aa-142f-447e-9a43-4404475ca4e0" alt="Example Image" width="800" height="400"/>

### 📝 6. 시사점
- 동적 크롤링을 진행하면서 HTML 태그들이 달라질 수 있음을 깨달았습니다. 이를 통해 try 문을 꼭 포함해야 한다는 점을 배웠습니다. 또한, 다음 크롤링 프로젝트에서는 API 사용과 일일 검색량을 미리 계산하여 작업을 계획하는 것이 중요하다는 것을 깨달았습니다.

- 네이버 뉴스의 HTML 태그는 대부분 동일하지만 한 번에 1000개까지만 크롤링이 가능하다는 제한이 있었습니다. 이에 따라, 다음 프로젝트에서는 유튜브나 인스타그램과 같이 한 번에 많은 데이터를 수집할 수 있는 플랫폼을 활용하고자 합니다.

- 정규표현식과 Spacy 같은 라이브러리를 사용해 텍스트 데이터 전처리 과정을 경험한 점이 매우 유익했습니다. 이는 앞으로 논문 구현 등에 자주 사용될 것으로 예상되는데, 미리 경험해본 것이 큰 도움이 되었습니다.
